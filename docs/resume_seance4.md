# #CoCoPySHS - Séance 4 - 08/06/2022

## Résumé de la séance

Cette quatrième séance propose un prolongement de la réflexion sur l'usage de Python dans le traitement de données avec un décalage sur deux aspects : à partir de deux projets impliquant la cartographie de réseaux, les intervenantes proposent de questionner la présence de l'usage d'une étape de script dans un projet plus général, ceci à partir de leur perspective de praticienne dans un cabinet de conseil en forte prise avec la science ouverte.

Maya Anderson-Gonzalez et Célya Gruson-Daniel sont des chercheuses praticiennes, et développent leur activité au sein d'Inno3, cabinet de conseil en innovation ouverte spécialiste des données ouvertes. Elles interviennent avec d'autres collaborateurs.rices sur de nombreux projets autour des pratiques des données, des enjeux juridiques à la conduite d'enquêtes. Une des particularités de leurs analyses est qu'elles ne sont pas elles-mêmes engagées dans l'écriture du code mais sont amenées à l'intégrer dans leurs projets. Elles s'appuient sur cette expérience pour développer une réflexion sur les méthodes mixtes intégrant des stratégies d'analyse du code ouvert.

Leur présentation vise justement à mettre ces stratégies de traitement de données avec la programmation en perspective de la conduite plus générale d'un projet : en effet, non seulement produire un script pour analyser ou visualiser les données n'est pas l'unique solution existante, mais peut aussi ne pas être la plus pertinente.

Les deux projets présentés portent sur des approches de cartographie de relations. Le premier se concentre sur les échanges Twitter pendant deux conférences (ChaosCon et FOSDEM), l'autres sur des liens entre site web autour des enjeux de la science ouverte dans le cadre du Laboratoire de la science ouverte. [rajouter les liens ici ?]

Dans le premier projet, l'approche déployée reposait sur les méthodes mixtes. Pour la partie d'analyse des tweets, elles ont fait appel aux services d'un data scientist extérieur qui a développé un script pour récupérer et mettre en forme l'analyse ultérieure qui a pris place dans Gephi (logiciel d'analyse de réseaux). L'enjeu central a alors été de co-développer le script avec cet interlocuteur extérieur puis de documenter chacune des étapes afin de les rendre lisibles et reproductibles. Le script en lui-même reposait sur l'outil développé en Python twark (ligne de commande) puis complété par un Notebook Jupyter. L'ensemble du code a ensuite été déposé sur un FramaGit. Les données brutes sont accessibles grâce à l'identifiant des tweets utilisés.

Dans le deuxième projet, le choix s'est porté sur une solution "no-code" en recourrant à la plateforme Hyphe développé par le Médialab de SciencesPo [https://hyphe.medialab.sciences-po.fr/], qui se trouve développée en Python. Il permet de collecter des sites et leurs liens à partir de mots-clés. Comme dans ce cas, l'interface est disponible avec des options à sélectionner et non pas automatisé par un script, il a été nécessaire de documenter l'ensemble des opérations réalisées (pas la possibilité de documenter le code en lui-même). Une fois les données collectées, de nouveau l'analyse a été réalisée avec Gephi.

La mise en comparaison de ces deux projets permet de réflechir sur de nombreuses dimensions : le choix d'une solution impliquant de la programmation vs. un logiciel déjà existant ; l'intégration d'une expertise extérieure dans une équipe pour développer le morceau de code nécessaire pour la collecte et la mise en forme des données ; les stratégies différentes de documentation des étapes de traitement des données. Célya et Maya identifient plusieurs points : recourir à un code explicite permet une plus grande transparence, mais la solution no-code est plus simple à déployer en l'absence d'expertise de programmation disponible. Le code est moins accessible, et conduit à créer plus de dépendance avec la personne qui programme ; cependant, il y a aussi une dépendance aux versions du logiciel utilisé qui peut fragiliser un projet. 

Dans ces deux cas, la programmation Python est présente par contre avec deux formes de coulisses:  d'une part, le script, sa co-production et les expertises nécessaires ; d'autre part, le travail d'ingénierie pour développer un logiciel dédié qui nécessite des ressources plus importantes (investies par le Médialab) et une communauté de développeurs.euses pour assurer son maintien. Dans les deux cas, elles insistent sur l'importance de relier l'opération de codage (même si elle est invisible) et de décodage, qui correspond à la documentation des traitements réalisés pour la transparence et la réutilisation. Dans les deux cas, les outils conduisent à une mise en vue des données. Et il apparaît nécessaire de développer une base de culture computationnelle pour pouvoir se mouvoir dans les outils possibles et initier une discussion avec des compétences extérieures.

La discussion s'ouvre sur cette coulisse du code qui est celle du maintien des outils utilisés par nos communautés. Que ce soit Hyphe, ou des bibliothèques Python comme scikit-learn, il est nécessaire d'apporter des ressources pour assurer le maintien, l'adaptation et la promotion de ces solutions. Cela passe par des politiques de soutien explicites de l'open source dans la recherche scientifique, et de valoriser les contributeurs à ces outils.

