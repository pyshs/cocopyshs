# #CoCoPySHS - Séance 4 - 08/06/2022

## Résumé de la séance

Cette quatrième séance propose un prolongement de la réflexion sur l'usage de Python dans le traitement de données avec un décalage sur deux aspects : 
1. à partir de deux projets impliquant la cartographie de réseaux, les intervenantes proposent de questionner la présence de l'usage d'une étape de script dans des études plus générales, 
2. à partir de leur perspective de praticienne dans un cabinet de conseil en forte prise avec la science ouverte.

Maya Anderson-Gonzalez et Célya Gruson-Daniel sont des chercheuses praticiennes, et développent leur activité au sein d'Inno3, cabinet de conseil en innovation ouverte spécialiste des données ouvertes, de l'open source. Elles interviennent avec d'autres collaborateurs.rices sur de nombreux projets autour des pratiques associées aux données en recherche et de leur ouverture, des enjeux juridiques et éthiques en lien avec la conduite d'enquêtes. Une des particularités de leurs usage de Python est qu'elles ne sont pas elles-mêmes engagées directement dans l'écriture du code mais sont amenées à l'intégrer dans leurs projets avec l'aide d'autres chercheurs et chercheuses. Elles s'appuient sur cette expérience pour développer une réflexion sur les méthodes mixtes intégrant des stratégies d'analyse et de visualisation dans une démarche de science ouverte. 

Leur présentation vise justement à mettre ces stratégies de traitement de données avec la programmation en perspective de la conduite plus générale d'un projet : en effet, non seulement produire un script pour analyser ou visualiser les données n'est pas l'unique solution existante, mais peut aussi ne pas être la plus pertinente.

Les deux projets présentés portent sur des approches de cartographie de relations. Le premier se concentre sur les échanges Twitter pendant deux conférences (ChaosCon et FOSDEM), l'autre sur des liens entre site web autour des enjeux de la recherche sur la recherche/metascience dans le cadre de la préfiguration du Laboratoire de la science ouverte du Comité pour la science ouverte : 
- OSOS : analyse d'un réseau d’acteurs à la croisée de l'Open Source et de l’Open Science [billet de blog](https://inno3.fr/actualite/projet-de-recherche-ouverte-osos-analyse-dun-reseau-dacteurs-la-croisee-de-lopen-source) + [Gitlab](https://code.inno3.cricket/ouvert/osos)
- LabSO : étude exploratoire sur la recherche sur la recherche :rapport sur [HAL](https://hal-lara.archives-ouvertes.fr/hal-03663434) +  [Gitlab](https://gitlab.com/inno3/labso-meta-research-public) + site web de l'étude [Pubpub](https://meta-open-research.pubpub.org/)

Dans le premier projet, l'approche déployée reposait sur les méthodes mixtes. Pour la partie d'analyse des tweets, elles ont fait appel aux services d'un data scientist extérieur qui a développé un script pour récupérer et mettre en forme l'analyse ultérieure qui a été menée dans [Gephi](https://gephi.org/) (logiciel d'analyse de réseaux). L'enjeu central a alors été de co-développer le script avec cet interlocuteur extérieur puis de documenter chacune des étapes afin de les rendre lisibles et reproductibles. Le script en lui-même reposait sur l'outil développé en Python twark (ligne de commande) puis complété par un Notebook Jupyter. L'ensemble du code a ensuite été déposé sur un FramaGit. Les données brutes sont accessibles grâce à l'identifiant des tweets utilisés.

Dans le deuxième projet, le choix s'est porté sur une solution "no-code" en recourrant à la [plateforme Hyphe](https://hyphe.medialab.sciences-po.fr/) développé par le Médialab de SciencesPo, qui se trouve développée en Python pour le backend. L'outil permet de collecter des sites et leurs liens à partir de quelques liens url initiaux. Dans ce cas, l'interface est disponible avec des options à sélectionner et n'est pas automatisée par un script, il a été donc important de de documenter l'ensemble des opérations réalisées (pas la possibilité de documenter le code en lui-même). Une fois les données collectées, de nouveau l'analyse de réseaux et l'obtention des visualisations ont été réalisées avec Gephi.

La mise en comparaison de ces deux projets permet de réflechir sur de nombreuses dimensions : le choix d'une solution impliquant de la programmation vs. un logiciel déjà existant ; l'intégration d'une expertise extérieure dans une équipe pour développer le morceau de code nécessaire pour la collecte et la mise en forme des données ; les stratégies différentes de documentation des étapes de traitement des données. Célya et Maya identifient plusieurs points : recourir à un code explicite permet une plus grande transparence, mais la solution "no-code" est plus simple à déployer en l'absence d'expertise de programmation disponible. Le code est moins accessible, et conduit à créer plus de dépendance avec la personne qui programme ; cependant, il y a aussi une dépendance aux versions du logiciel utilisé qui peut fragiliser un projet. 

Dans ces deux cas, la programmation Python est présente par contre avec deux formes de coulisses:  d'une part, le script, sa co-production et les expertises nécessaires ; d'autre part, le travail d'ingénierie pour développer un logiciel dédié qui nécessite des ressources plus importantes (investies par le Médialab) et une communauté de développeurs.euses pour assurer son maintien. Dans les deux cas, elles insistent sur l'importance de relier l'opération de codage (même si elle est invisible) et de décodage, qui correspond à la documentation des traitements réalisés pour la transparence et la réutilisation. Dans les deux cas, les outils conduisent à une mise en vue des données. Et il apparaît nécessaire de développer une base de culture computationnelle pour pouvoir se mouvoir dans les outils possibles et initier une discussion avec des compétences extérieures.

La discussion s'ouvre sur cette coulisse du code qui est celle du maintien des logiels et codes sources utilisés par nos communautés. Que ce soit Hyphe, ou des bibliothèques Python comme scikit-learn, il est nécessaire d'apporter des ressources pour assurer le maintien, l'adaptation et la promotion de ces solutions. Cela passe par des politiques de soutien explicites de l'open source dans la recherche scientifique (par exemple [le Plan National pour la science ouverte 2](https://www.ouvrirlascience.fr/deuxieme-plan-national-pour-la-science-ouverte/) , et des modalités adaptées de valorisation des logiciels libres et open source en recherche tout autant que des communautés qui y contribuent.

